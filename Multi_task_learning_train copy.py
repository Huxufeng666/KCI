import os
os.environ["MPLBACKEND"] = "Agg"
import random, datetime, csv
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from scipy.ndimage import distance_transform_edt

# ------------------ ä½ çš„æ¨¡å— ------------------
from get_data import BUSI_Data 
from model.FPNUent_Multi_task import MultiTaskFPNUNet
from utils.tools import visualize_batch

# import csv
from  get_data  import  BUSI_Data ,MedicalDataset
from model.model import EndToEndModel
from model.ConvNeXt_Small_FPN_CBAM import EndToEndModel2
from model.dpt_asf_gan import DPT_ASF_GAN
from tools import plot_loss_curve
from model.FPNUNet import FPNUNet_CBAM_Residual
from model.Swin_unet import SwinUnet
from model.AAUnet import AAUNet
from model.U_net_plus import UNetPlusPlus
from scipy.ndimage import distance_transform_edt



from model.U_net import UNet


# ==================== æ—©åœæœºåˆ¶ç±» (æ–°å¢!) ====================
class EarlyStopping:
    """
    å½“éªŒè¯é›†æŒ‡æ ‡ï¼ˆå¦‚ Diceï¼‰åœ¨ patience è½®æ¬¡å†…æ²¡æœ‰æå‡æ—¶ï¼Œåœæ­¢è®­ç»ƒã€‚
    """
    def __init__(self, patience=20, verbose=False, delta=0):
        """
        Args:
            patience (int): ä¸Šæ¬¡æŒ‡æ ‡æå‡åï¼Œç­‰å¾…å¤šå°‘è½®ï¼ˆé»˜è®¤ 20ï¼‰ã€‚
            verbose (bool): æ˜¯å¦æ‰“å°æ—¥å¿—ã€‚
            delta (float): æŒ‡æ ‡è¢«è®¤ä¸ºæå‡çš„æœ€å°å˜åŒ–é‡ã€‚
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_score_max = -np.inf
        self.delta = delta

    def __call__(self, val_score):
        # è¿™é‡Œçš„ val_score æ˜¯ Diceï¼Œè¶Šå¤§è¶Šå¥½
        score = val_score

        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0


# ==================== æŸå¤±å‡½æ•° ====================
class BCEDiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super().__init__()
        self.smooth = smooth
        self.bce = nn.BCEWithLogitsLoss()

    def forward(self, logits, targets):
        bce_loss = self.bce(logits, targets)
        prob = torch.sigmoid(logits).flatten(1)
        targets = targets.flatten(1)
        inter = (prob * targets).sum(1)
        union = prob.sum(1) + targets.sum(1)
        dice = (2. * inter + self.smooth) / (union + self.smooth)
        return bce_loss + (1 - dice.mean())

def dice_coeff(logits, targets, smooth=1e-6):
    prob = torch.sigmoid(logits).flatten(1)
    targets = targets.flatten(1)
    inter = (prob * targets).sum(1)
    union = prob.sum(1) + targets.sum(1)
    return ((2. * inter + smooth) / (union + smooth)).mean()

# å…¨å±€ Loss å®ä¾‹
seg_criterion = BCEDiceLoss()
ce_cls  = nn.CrossEntropyLoss()
bce_edge = nn.BCEWithLogitsLoss()
l1_rec  = nn.L1Loss()

# ==================== è®¾ç½®éšæœºç§å­ ====================
def set_seed(seed=2025):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ==================== æ ¸å¿ƒï¼šå•æ¬¡å®éªŒè¿è¡Œå‡½æ•° ====================
def run_experiment(exp_name, config):
    print(f"\n{'='*20} Start Experiment: {exp_name} {'='*20}")
    print(f"Config: {config}")
    
    set_seed(2025) # ä¿è¯æ¯ç»„å®éªŒåˆå§‹æƒé‡ä¸€è‡´ï¼Œå…¬å¹³æ¯”è¾ƒ

    # 1. è®¾ç½®è·¯å¾„
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"results/ablation/{exp_name}_{ts}"
    os.makedirs(log_dir, exist_ok=True)
    log_csv = os.path.join(log_dir, "log.csv")
    
    with open(log_csv, "w", newline="") as f:
        csv.writer(f).writerow(["epoch", "train_loss", "val_loss", "val_dice", "high_conf_dice", "cls_acc", "lr"])

    # 2. æ•°æ®ä¸æ¨¡å‹
    IMG_SIZE = 256
    BATCH_SIZE = 4 # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_set = BUSI_Data(root_dir="/workspace/dataset", split="train", img_size=IMG_SIZE)
    val_set   = BUSI_Data(root_dir="/workspace/dataset", split="val",   img_size=IMG_SIZE)
    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0)
    val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # model = UNetPlusPlus(in_ch=1, seg_ch=1, num_classes=3).to(device)
    model = UNet().to(device)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    
    
    early_stopping = EarlyStopping(patience=20, verbose=True)
    
    # 3. è®­ç»ƒå‚æ•°
    EPOCHS = 150 # æ¶ˆèå®éªŒå¯ä»¥é€‚å½“å‡å°‘è½®æ•°ï¼Œæ¯”å¦‚100-150è½®çœ‹è¶‹åŠ¿
    SEG_START_EPOCH = 6
    CONF_THRESH = 0.95
    best_val_dice = 0.0

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    for epoch in range(1, EPOCHS + 1):
        model.train()
        epoch_loss = 0.0
        
        for img, mask, cls_label, edge_gt, recon_gt in tqdm(train_loader, desc=f"[{exp_name}] Ep {epoch}", leave=False):
            img = img.to(device)
            mask = mask.to(device).float()
            cls_label = cls_label.to(device).long()
            edge_gt = edge_gt.to(device).float()
            recon_gt = recon_gt.to(device)

            optimizer.zero_grad()
            seg_main, aux2, aux3, aux4, cls_logit, edge_out, recon_out = model(img)

            total_loss = torch.tensor(0.0, device=device)

            # ------------------ A. è¾…åŠ©ä»»åŠ¡ Loss (æ ¹æ® Config å¼€å…³) ------------------
            if config['use_cls']:
                total_loss += ce_cls(cls_logit, cls_label)
            
            if config['use_edge']:
                total_loss += bce_edge(edge_out, edge_gt)
            
            if config['use_recon']:
                total_loss += 0.02 * l1_rec(recon_out, recon_gt)

            # ------------------ B. ä¸»ä»»åŠ¡ Segmentation Loss ------------------
            # ç­–ç•¥ï¼šå¦‚æœæœ‰ Cls ä¸”å¼€å¯äº†ç½®ä¿¡åº¦ç­›é€‰ï¼Œåˆ™ä½¿ç”¨ç­›é€‰é€»è¾‘ï¼›å¦åˆ™å…¨é‡è®­ç»ƒ
            should_train_seg = False
            high_conf_mask = torch.ones(img.size(0), 1, 1, 1).to(device) # é»˜è®¤å…¨é€‰

            if config['use_cls']:
                # åŸæœ‰é€»è¾‘ï¼šå‰å‡ è½®ä¸è®­åˆ†å‰²ï¼Œåé¢æŒ‰ç½®ä¿¡åº¦è®­
                if epoch >= SEG_START_EPOCH:
                    with torch.no_grad():
                        prob = F.softmax(cls_logit, dim=1)
                        confidence, _ = torch.max(prob, dim=1)
                        high_conf_mask = (confidence >= CONF_THRESH).float().view(-1, 1, 1, 1)
                        if high_conf_mask.sum() > 0:
                            should_train_seg = True
            else:
                # æ—  Cls æ—¶ï¼šå§‹ç»ˆè®­ç»ƒåˆ†å‰² (ä¹Ÿå¯ä»¥ä¿ç•™ warmupï¼Œè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥è®­)
                should_train_seg = True

            if should_train_seg:
                H, W = mask.shape[-2:]
                # ä¸Šé‡‡æ ·
                seg_up = F.interpolate(seg_main, size=(H,W), mode='bilinear', align_corners=False)
                aux2_up = F.interpolate(aux2, size=(H,W), mode='bilinear', align_corners=False)
                aux3_up = F.interpolate(aux3, size=(H,W), mode='bilinear', align_corners=False)
                aux4_up = F.interpolate(aux4, size=(H,W), mode='bilinear', align_corners=False)

                # è®¡ç®—åŸºç¡€åˆ†å‰² Loss
                l_main = seg_criterion(seg_up, mask)
                l_aux = 0.8*seg_criterion(aux4_up, mask) + 0.6*seg_criterion(aux3_up, mask) + 0.4*seg_criterion(aux2_up, mask)
                
                # å¦‚æœæœ‰ Clsï¼Œåº”ç”¨ Mask ç­›é€‰ (åªè®¡ç®—é«˜ç½®ä¿¡åº¦æ ·æœ¬)
                if config['use_cls']:
                    total_loss += (l_main + l_aux) # ç®€åŒ–ï¼šåªè¦æœ‰æ ·æœ¬è¿‡çº¿ï¼Œå°±åŠ è¿™ä¸ª Loss
                else:
                    total_loss += (l_main + l_aux)

            # Backprop
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            epoch_loss += total_loss.item()

        # ==================== éªŒè¯å¾ªç¯ ====================
        model.eval()
        val_losses = []
        val_dice_all = []
        val_dice_high = []
        cls_correct = 0
        total_samples = 0

        with torch.no_grad():
            for img, mask, cls_gt, edge_gt, recon_gt in val_loader:
                img = img.to(device); mask = mask.to(device).float(); cls_gt = cls_gt.to(device).long()
                edge_gt = edge_gt.to(device).float(); recon_gt = recon_gt.to(device)

                seg_main, _, _, _, cls_logit, edge_out, recon_out = model(img)
                H, W = img.shape[-2:]
                seg_main = F.interpolate(seg_main, size=(H,W), mode='bilinear', align_corners=False)

                # è®¡ç®—éªŒè¯ Loss (ä»…ä¾›å‚è€ƒï¼Œä¸å½±å“æŒ‡æ ‡)
                v_loss = BCEDiceLoss()(seg_main, mask) # å§‹ç»ˆè®¡ç®—åˆ†å‰² Loss
                if config['use_cls']: v_loss += 0.3 * ce_cls(cls_logit, cls_gt)
                if config['use_edge']: v_loss += bce_edge(F.interpolate(edge_out, size=(H,W)), edge_gt)
                if config['use_recon']: v_loss += 0.02 * l1_rec(F.interpolate(recon_out, size=(H,W)), img)
                
                val_losses.append(v_loss.item())
                val_dice_all.append(dice_coeff(seg_main, mask).item())

                # è®°å½•åˆ†ç±»å‡†ç¡®ç‡
                if config['use_cls']:
                    cls_correct += (cls_logit.argmax(1) == cls_gt).sum().item()
                    
                    # è®°å½•é«˜ç½®ä¿¡æ ·æœ¬ Dice
                    prob = F.softmax(cls_logit, dim=1)
                    conf, _ = torch.max(prob, dim=1)
                    high_idx = conf >= 0.95
                    if high_idx.sum() > 0:
                        val_dice_high.append(dice_coeff(seg_main[high_idx], mask[high_idx]).item())
                
                total_samples += cls_gt.size(0)

        # ç»Ÿè®¡æŒ‡æ ‡
        avg_val_loss = np.mean(val_losses)
        avg_dice = np.mean(val_dice_all)
        high_dice = np.mean(val_dice_high) if val_dice_high else 0.0
        cls_acc = cls_correct / total_samples if config['use_cls'] else 0.0
        
        # ä¿å­˜æœ€ä½³ Dice æ¨¡å‹
        if avg_dice > best_val_dice:
            best_val_dice = avg_dice
            torch.save(model.state_dict(), os.path.join(log_dir, "best_model.pth"))
            print(f" âœ¨ New Best Dice: {best_val_dice:.4f}")

        # ==================== æ ¸å¿ƒä¿®æ”¹ï¼šæ—©åœæ£€æŸ¥ ====================
        early_stopping(avg_dice) # ä¼ å…¥è¦ç›‘æ§çš„æŒ‡æ ‡ï¼Œè¿™é‡Œæ˜¯ Dice
        if early_stopping.early_stop:
            print(f"ğŸ›‘ Early stopping triggered at epoch {epoch}")
            break # è·³å‡º epoch å¾ªç¯ï¼Œç»“æŸå½“å‰å®éªŒ
        
        
        # è®°å½•æ—¥å¿—
        with open(log_csv, "a", newline="") as f:
            csv.writer(f).writerow([epoch, f"{epoch_loss/len(train_loader):.4f}", f"{avg_val_loss:.4f}", 
                                    f"{avg_dice:.4f}", f"{high_dice:.4f}", f"{cls_acc:.4f}", 
                                    f"{optimizer.param_groups[0]['lr']:.2e}"])
        
        scheduler.step(avg_val_loss)

    print(f"âœ… Experiment {exp_name} Finished. Best Dice: {best_val_dice:.4f}")
    torch.cuda.empty_cache()

# ==================== ä¸»å…¥å£ï¼šæ‰§è¡Œ 5 ç»„å®éªŒ ====================
if __name__ == "__main__":
    
    # å®šä¹‰ 5 ç»„å®éªŒé…ç½®
    experiments = {
        # Exp 1: Baseline (çº¯åˆ†å‰²)
        "Exp1_Baseline": {'use_cls': False, 'use_edge': False, 'use_recon': False},
        
        # Exp 2: + Classification
        "Exp2_with_Cls": {'use_cls': True,  'use_edge': False, 'use_recon': False},
        
        # Exp 3: + Edge
        "Exp3_with_Edge":{'use_cls': False, 'use_edge': True,  'use_recon': False},
        
        # Exp 4: + Recon
        "Exp4_with_Recon":{'use_cls': False, 'use_edge': False, 'use_recon': True},
                       
        # Exp 5: + Recon + Edge
        "Exp5_with_Recon":{'use_cls': False, 'use_edge': True, 'use_recon': True},
               
        # Exp 6: + Recon+ Classification
        "Exp6_with_Recon":{'use_cls': True, 'use_edge': False, 'use_recon': True},
                
        # Exp 7: + Edge + Classification
        "Exp7_with_Recon":{'use_cls': True, 'use_edge': True, 'use_recon': True},
           
        # Exp 8: Full Proposed (å…¨éƒ¨å¼€å¯)
        "Exp8_Full_Model":{'use_cls': True,  'use_edge': True,  'use_recon': False},
    }

    # ä½ å¯ä»¥é€‰æ‹©è¿è¡Œå…¨éƒ¨ï¼Œæˆ–è€…åªè¿è¡ŒæŸä¸€ä¸ª
    # for name, config in experiments.items():
    #     run_experiment(name, config)
    
    # ä¾‹å¦‚ï¼šåªè¿è¡Œ Exp 1
    run_experiment("Unet", experiments["Exp1_Baseline"])
    # è¿è¡Œ Exp 2
    # run_experiment("Exp2_with_Cls", experiments["Exp2_with_Cls"])
    # è¿è¡Œ Exp 3
    # run_experiment("Exp3_with_Edge", experiments["Exp3_with_Edge"])

    # è¿è¡Œ Exp 4
    # run_experiment("Exp4_with_Recon", experiments["Exp4_with_Recon"])
    # è¿è¡Œ Exp 5
    # run_experiment("Exp5_with_Recon", experiments["Exp5_with_Recon"])
    # è¿è¡Œ Exp 6        
    # run_experiment("Exp6_with_Recon", experiments["Exp6_with_Recon"])
    # è¿è¡Œ Exp 7
    # run_experiment("Exp7_with_Recon", experiments["Exp7_with_Recon"])
    # è¿è¡Œ Exp 8
    # run_experiment("Exp8_Full_Model", experiments["Exp8_Full_Model"])