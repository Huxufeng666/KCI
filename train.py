
import os
import random
import numpy as np
import datetime
import csv
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.utils as vutils
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau

# import csv
from  get_data  import  BUSI_Data 
from model.model import EndToEndModel
from model.ConvNeXt_Small_FPN_CBAM import EndToEndModel2
from model.dpt_asf_gan import DPT_ASF_GAN
from tools import plot_loss_curve
from model.FPNUNet import FPNUNet_CBAM_Residual
from model.Swin_unet import SwinUnet
from model.AAUnet import AAUNet
from model.U_net_plus import UNetPlusPlus
from scipy.ndimage import distance_transform_edt



from model.U_net import UNet



# ==================================================
# 5.1 æ—©åœæœºåˆ¶ç±»
# ==================================================
class EarlyStopping:
    """
    æ—©åœæœºåˆ¶ï¼šå½“éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶ï¼Œåœæ­¢è®­ç»ƒ
    """
    def __init__(self, patience=10, min_delta=1e-6, restore_best_weights=True):
        """
        Args:
            patience (int): éªŒè¯æŸå¤±æœªæ”¹å–„æ—¶ï¼Œç­‰å¾…çš„è½®æ•°
            min_delta (float): æœ€å°æ”¹å–„é˜ˆå€¼
            restore_best_weights (bool): æ˜¯å¦æ¢å¤æœ€ä¼˜æƒé‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        self.early_stop = False

    def step(self, val_loss, model):
        """
        æ¯è½®éªŒè¯åè°ƒç”¨
        """
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_weights = {k: v.cpu() for k, v in model.state_dict().items()}
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.best_weights = {k: v.cpu() for k, v in model.state_dict().items()}
            print(f"Validation loss improved to {val_loss:.6f}")
        else:
            self.counter += 1
            print(f"No improvement. Early stopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                print(f"Early stopping triggered after epoch {epoch}")
                self.early_stop = True
                if self.restore_best_weights:
                    print("Restoring best weights...")
                    model.load_state_dict(self.best_weights)





# ==================================================
# 0. CUDA è®¾ç½®
# ==================================================
def init_cuda():
    """åˆå§‹åŒ–CUDAè®¾ç½®"""
    # ç¦ç”¨NCCLç›¸å…³è®¾ç½®
    os.environ['NCCL_DEBUG'] = 'WARN'  # é™ä½NCCLæ—¥å¿—çº§åˆ«
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAæ“ä½œ
    
    if torch.cuda.is_available():
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        # ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        # è®¾ç½®å†…å­˜åˆ†é…å™¨
        torch.cuda.set_per_process_memory_fraction(0.8)  # ä½¿ç”¨80%çš„å¯ç”¨GPUå†…å­˜
        
        # æ‰“å°GPUä¿¡æ¯
        print(f"Found {torch.cuda.device_count()} CUDA devices")
        for i in range(torch.cuda.device_count()):
            print(f"  Device {i}: {torch.cuda.get_device_name(i)}")
            print(f"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")

init_cuda()

# ==================================================
# 1. å›ºå®šéšæœºç§å­ï¼ˆå®Œå…¨å¯å¤ç°ï¼‰
# ==================================================
def set_seed(seed: int = 2025):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # ç¡®å®šæ€§è®¾ç½®
    torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True,warn_only=True)

    def seed_worker(worker_id):
        worker_seed = seed + worker_id
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    return seed_worker, torch.Generator().manual_seed(seed)


# ==================================================
# 2. æ•°æ®é›† & DataLoader
# ==================================================
set_seed(2025)

# ==================================================
train_data = GetData(
    image_dir='dataset/train/images',
    mask_dir='dataset/train/masks'
)
val_data = GetData(
    image_dir='dataset/val/images',
    mask_dir='dataset/val/masks'
)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
batch_size = 16



device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# å‡å°batch sizeå’Œworkeræ•°é‡ä»¥é™ä½å†…å­˜å‹åŠ›
# è¿›ä¸€æ­¥å‡å°batch sizeä»¥é™ä½å†…å­˜å‹åŠ›
batch_size = 16

train_loader = DataLoader(
    train_data, 
    batch_size=batch_size, 
    shuffle=True,
    num_workers=0,  # æš‚æ—¶ä¸ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True if torch.cuda.is_available() else False,
    drop_last=True,
    # worker_init_fn=seed_worker, 
    # generator=g,
)

val_loader = DataLoader(
    val_data, 
    batch_size=batch_size,
    shuffle=False,
    num_workers=0,  # æš‚æ—¶ä¸ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True if torch.cuda.is_available() else False,
    drop_last=False,
    # worker_init_fn=seed_worker, 
    # generator=g,
)


# ==================================================
# 3. æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
# ==================================================
# é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„GPU
if torch.cuda.is_available():
    torch.cuda.set_device(0)  # å¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    print("Using CPU")

model = EndToEndModel2(in_channels=1, num_classes=1).to(device)
# model = FPNUNet_CBAM_Residual().to(device)

print(f"Model created and moved to {device}")

model_name =  model.__class__.__name__   # ç›´æ¥ä½¿ç”¨ç±»åè€Œä¸æ˜¯ä»wrapped modelè·å–
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = f"results/{model_name}_{timestamp}"
os.makedirs(log_dir, exist_ok=True)


class BCEWithLogitsLossWithSmoothing(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, logits, targets):
        targets = targets * (1.0 - self.smoothing) + 0.5 * self.smoothing
        return self.bce(logits, targets).mean()


class ComboLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.5, gamma=2.0):
        super().__init__()
        self.alpha = alpha  # BCE æƒé‡
        self.beta = beta    # Dice æƒé‡
        self.gamma = gamma  # Focal

    def forward(self, logits, targets):
        # BCE with Focal
        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='mean')
        prob = torch.sigmoid(logits)
        focal_weight = (1 - prob) ** self.gamma * targets + prob ** self.gamma * (1 - targets)
        bce = (focal_weight * F.binary_cross_entropy_with_logits(logits, targets, reduction='none')).mean()

        # Dice
        smooth = 1e-6
        intersection = (prob * targets).sum(dim=(2,3))
        dice = 1 - (2 * intersection + smooth) / (prob.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + smooth)
        dice = dice.mean()

        return self.alpha * bce + self.beta * dice



def dice_loss_per_sample(logits, masks, bce_weight=1.0, dice_weight=1.0, smooth=1e-6):
    """
    æ›¿æ¢ç‰ˆï¼šBCE + Dice æ··åˆæŸå¤±ï¼ˆæ¨èï¼ï¼‰
    åªæ”¹è¿™ä¸€æ®µï¼Œæ‰€æœ‰å…¶ä»–ä»£ç ä¸åŠ¨ï¼
    """
    # 1. BCE éƒ¨åˆ†ï¼ˆä½¿ç”¨ logitsï¼Œæ•°å€¼ç¨³å®šï¼‰
    bce_loss = nn.BCEWithLogitsLoss()(logits, masks)

    # 2. Dice éƒ¨åˆ†
    probs = torch.sigmoid(logits)
    B = probs.shape[0]
    probs_flat = probs.view(B, -1)
    masks_flat = masks.view(B, -1)

    inter = (probs_flat * masks_flat).sum(dim=1)
    union = probs_flat.sum(dim=1) + masks_flat.sum(dim=1)
    dice = (2 * inter + smooth) / (union + smooth)
    dice_loss = 1 - dice.mean()  # æ ‡é‡

    # 3. æ··åˆ
    return bce_weight * bce_loss + dice_weight * dice_loss



class BCEDiceLoss(nn.Module):
    def __init__(self, bce_weight=1.0, dice_weight=1.0, smooth=1e-6):
        super().__init__()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
        self.smooth = smooth
        self.bce = nn.BCEWithLogitsLoss()  # ç›´æ¥åƒ logitsï¼Œç¨³å®šï¼

    def forward(self, logits, masks):
        # 1. BCE éƒ¨åˆ†ï¼ˆå¸¦ logitsï¼Œæ•°å€¼ç¨³å®šï¼‰
        bce_loss = self.bce(logits, masks)

        # 2. Dice éƒ¨åˆ†ï¼ˆåŠ  log é˜²æ¢¯åº¦æ¶ˆå¤±ï¼‰
        probs = torch.sigmoid(logits)
        B, _, H, W = probs.shape
        probs_flat = probs.view(B, -1)
        masks_flat = masks.view(B, -1)

        inter = (probs_flat * masks_flat).sum(dim=1)
        union = probs_flat.sum(dim=1) + masks_flat.sum(dim=1)
        dice = (2 * inter + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice.mean()  # æ ‡é‡ï¼

        # å¯é€‰ï¼šåŠ  log å¢å¼ºå°ç›®æ ‡æ¢¯åº¦
        # dice_loss = -torch.log(dice.clamp_min(1e-6)).mean()

        return self.bce_weight * bce_loss + self.dice_weight * dice_loss



bce_loss = BCEWithLogitsLossWithSmoothing(smoothing=0.1)
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)


# =========================
# =========================
# 4. æ—¥å¿—æ–‡ä»¶
# ==================================================
log_csv = os.path.join(log_dir, f"log_{model_name}_{timestamp}.csv")
model_path = os.path.join(log_dir, f"best_{model_name}_{timestamp}.pth")
loss_plot_path = os.path.join(log_dir, f"loss_plot_{model_name}_{timestamp}.png")
image_save_template = os.path.join(log_dir, "epoch{}_{}.png".format("{:03d}", model_name))

with open(log_csv, mode="w", newline="") as f:
    
    writer = csv.writer(f)
    writer.writerow(["# Hyperparameters"])
    writer.writerow(["Model name", model_name])
    writer.writerow(["Time", timestamp])
    writer.writerow(["Batch size", train_loader.batch_size])
    writer.writerow(["Learning rate", optimizer.param_groups[0]['lr']])
    writer.writerow(["Loss function", "BCEWithLogitsLossWithSmoothing + dice_loss_per_sample"])
    writer.writerow(["Optimizer", type(optimizer).__name__])
    writer.writerow(["Scheduler", type(scheduler).__name__])
    writer.writerow([])
    writer.writerow(["epoch", "train_loss", "val_loss", "learning_rate"])


# ==================================================
# 5. è®­ç»ƒå¾ªç¯
# ==================================================
num_epochs = 100
best_val_loss = float("inf")

early_stopping = EarlyStopping(patience=15, min_delta=1e-6, restore_best_weights=True)


def train_tta(x, enable=False):
    if not enable:
        return x
    if random.random() > 0.5:
        x = torch.flip(x, [3])
    if random.random() > 0.5:
        x = torch.rot90(x, 1, [2, 3])
    return x

def boundary_loss(logits, masks, device='cpu'):

    probs = torch.sigmoid(logits)  
    device = logits.device# [B,1,H,W] -> [0,1]
    B, _, H, W = masks.shape
    
    dist_maps = []
    # 1. å…ˆæŠŠ mask è½¬åˆ° CPU + numpyï¼ˆEDT åªæ¥å— numpyï¼‰
    masks_np = masks.cpu().numpy()               # [B,1,H,W]
    
    for i in range(B):
        # å–é˜ˆå€¼ >0.5 çš„äºŒå€¼å›¾ï¼ˆuint8 èƒ½åŠ é€Ÿ EDTï¼‰
        binary = (masks_np[i, 0] > 0.5).astype(np.uint8)
        # è·ç¦»å˜æ¢ï¼šèƒŒæ™¯ï¼ˆ0ï¼‰åˆ°æœ€è¿‘å‰æ™¯ï¼ˆ1ï¼‰çš„è·ç¦»
        dist = distance_transform_edt(1 - binary)   # è¿™é‡Œ 1-binary ä¿è¯å‰æ™¯ä¸º 0ï¼ˆEDT è®¡ç®—å‰æ™¯åˆ°èƒŒæ™¯çš„è·ç¦»ï¼‰
        # å½’ä¸€åŒ–åˆ° [0,1]ï¼Œé˜²æ­¢é™¤ä»¥ 0
        dist = dist / (dist.max() + 1e-6)
        dist_maps.append(dist)
    
    # 2. å †å æˆ [B,H,W]ï¼Œè½¬å› torch å¹¶æ”¾åˆ°åŸè®¾å¤‡
    dist_map = torch.from_numpy(np.stack(dist_maps)).unsqueeze(1).to(device, dtype=torch.float32)
    # dist_map: [B,1,H,W] ä¸ probs å¯¹é½
    
    return F.mse_loss(probs, dist_map)





top_k = 3
saved_models = []

for epoch in range(1, num_epochs + 1):
    # ----------- è®­ç»ƒ -----------
    model.train()
    total_train_loss = 0.0
    for batch_idx, (imgs, masks) in enumerate(tqdm(train_loader, desc=f"[Train] Epoch {epoch}")):
        try:
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®å¹¶ç§»åŠ¨åˆ°GPU
            imgs = imgs.float().to(device, non_blocking=True)
            masks = masks.float().to(device, non_blocking=True)
            imgss = train_tta(imgs,enable=False).to(device)  # è®­ç»ƒæ—¶ TTA
            
                        
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶
            segmentation,aux2, aux3, aux4 = model(imgs)
            
            # å°†åˆ†å‰²ç»“æœä¸Šé‡‡æ ·åˆ°ä¸è¾“å…¥ç›¸åŒçš„å°ºå¯¸
            if segmentation.shape[-2:] != imgs.shape[-2:]:
                segmentation = F.interpolate(
                    segmentation,
                    size=imgs.shape[-2:],
                    mode='bilinear',
                    align_corners=False
                )
                
            # å…³é”®ï¼šè¾…åŠ©å¤´ä¹Ÿå¿…é¡»ä¸Šé‡‡æ ·åˆ°åŸå›¾å°ºå¯¸ï¼ï¼
            aux2 = F.interpolate(aux2, size=imgs.shape[-2:], mode='bilinear', align_corners=False)
            aux3 = F.interpolate(aux3, size=imgs.shape[-2:], mode='bilinear', align_corners=False)
            aux4 = F.interpolate(aux4, size=imgs.shape[-2:], mode='bilinear', align_corners=False)

            # è®¡ç®—å„å¤´æŸå¤±ï¼ˆæ¨è ComboLoss æˆ–ä½ ç°æœ‰çš„ dice+bceï¼‰
            loss_main = 0.5 * bce_loss(segmentation, masks) + 0.5 * dice_loss_per_sample(segmentation, masks, bce_weight=1.0, dice_weight=1.0)
            loss_a2   = 0.5 * bce_loss(aux2, masks)         + 0.5 * dice_loss_per_sample(aux2, masks, bce_weight=1.0, dice_weight=1.0)
            loss_a3   = 0.5 * bce_loss(aux3, masks)         + 0.5 * dice_loss_per_sample(aux3, masks, bce_weight=1.0, dice_weight=1.0)
            loss_a4   = 0.5 * bce_loss(aux4, masks)         + 0.5 * dice_loss_per_sample(aux4, masks, bce_weight=1.0, dice_weight=1.0)
                
        
            boundary = 0.8 * boundary_loss(imgss, masks) if 'boundary_loss' in globals() else 0

    
            # è®¡ç®—æŸå¤±
            # loss_bce = bce_loss(segmentation, masks)
            # # loss_dice = dice_loss_per_sample(segmentation, masks).mean()
            # loss_dice = dice_loss_per_sample(segmentation, masks, bce_weight=1.0, dice_weight=1.0)
            # loss = 0.5 * loss_bce +  0.5 * loss_dice + 0.8 * boundary_loss(imgss,masks)
            
            
                    # ç»ˆææ·±ç›‘ç£æƒé‡ï¼ˆæœ€æ·±å±‚æƒé‡æœ€å¤§ï¼å®æµ‹æœ€å¼ºï¼‰
            loss = (loss_main + 
                    0.8 * loss_a4 +    # æœ€æ·±å±‚ï¼Œæƒé‡æœ€å¤§
                    0.6 * loss_a3 + 
                    0.4 * loss_a2) 
            
                
                    
            
            if not torch.isfinite(loss):
                print(f'è­¦å‘Š: æ£€æµ‹åˆ°éæœ‰é™æŸå¤±å€¼ï¼Œè·³è¿‡æœ¬æ‰¹æ¬¡ (batch {batch_idx})')
                continue
                
        except RuntimeError as e:
            print(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
            if "out of memory" in str(e):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                print("GPUå†…å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥batch")
                continue
            else:
                raise e
        
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item() * imgs.size(0)

    avg_train = total_train_loss / len(train_loader.dataset)

    # ----------- éªŒè¯ -----------
    model.eval()
    total_val_loss = 0.0
    with torch.no_grad():
        for imgs, masks in tqdm(val_loader, desc=f"[ Val ] Epoch {epoch}"):
            imgs = imgs.float().to(device)
            masks = masks.float().to(device)
            segmentation,aux2, aux3, aux4 = model(imgs)
            
            if segmentation.shape[-2:] != imgs.shape[-2:]:
                segmentation = F.interpolate(segmentation, size=imgs.shape[-2:], mode='bilinear', align_corners=False)
            
                # å…³é”®ï¼šè¾…åŠ©å¤´ä¹Ÿå¿…é¡»ä¸Šé‡‡æ ·åˆ°åŸå›¾å°ºå¯¸ï¼ï¼
                aux2 = F.interpolate(aux2, size=imgs.shape[-2:], mode='bilinear', align_corners=False)
                aux3 = F.interpolate(aux3, size=imgs.shape[-2:], mode='bilinear', align_corners=False)
                aux4 = F.interpolate(aux4, size=imgs.shape[-2:], mode='bilinear', align_corners=False)

                # è®¡ç®—å„å¤´æŸå¤±ï¼ˆæ¨è ComboLoss æˆ–ä½ ç°æœ‰çš„ dice+bceï¼‰
                pred = torch.sigmoid(segmentation)
                pred_aux = torch.sigmoid(aux2 + aux3 + aux4 * 1.5)  # è¾…åŠ©å¤´èåˆå¢å¼º
                pred_final = (pred + pred_aux) / 2   # èåˆä¸»å¤´å’Œè¾…åŠ©å¤´
                    
                                        
                    
            # å°†åˆ†å‰²ç»“æœä¸Šé‡‡æ ·åˆ°ä¸è¾“å…¥ç›¸åŒçš„å°ºå¯¸
            if segmentation.shape[-2:] != imgs.shape[-2:]:
                segmentation = F.interpolate(
                    segmentation,
                    size=imgs.shape[-2:],
                    mode='bilinear',
                    
                    align_corners=False
                )
                
            
            # loss_bce = bce_loss(segmentation, masks)
            
            # loss_dice = dice_loss_per_sample(segmentation, masks).mean()
            
            
            # loss = 0.5 * loss_bce + 0.8 * loss_dice
            
            loss = 0.5 * bce_loss(segmentation, masks) + 0.5 * dice_loss_per_sample(segmentation, masks).mean()
            

            total_val_loss += loss.item() * imgs.size(0)

    avg_val = total_val_loss / len(val_loader.dataset)

    print(f"Epoch {epoch:02d} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}")

    # å†™æ—¥å¿—
    with open(log_csv, mode="a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([epoch, f"{avg_train:.6f}", f"{avg_val:.6f}", optimizer.param_groups[0]['lr']])

    # ä¿å­˜æœ€ä¼˜æ¨¡å‹
    model_file = os.path.join(log_dir, f"model_epoch{epoch}_{avg_val:.4f}.pth")
    torch.save(model.state_dict(), model_file)
    saved_models.append((avg_val, model_file))
    saved_models.sort(key=lambda x: x[0])

    if len(saved_models) > top_k:
        _, to_delete = saved_models.pop(-1)
        if os.path.exists(to_delete):
            os.remove(to_delete)
            print(f"ğŸ—‘ï¸ Deleted old model: {to_delete}")

    if avg_val < best_val_loss:
        best_val_loss = avg_val


    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(avg_val)

    # æ—©åœåˆ¤æ–­
    early_stopping.step(avg_val, model)
    if early_stopping.early_stop:
        print(f"Training stopped early at epoch {epoch}")
        break

    # æ¯10è½®ä¿å­˜å¯è§†åŒ–å›¾
    if epoch % 10 == 0:
        sample_imgs, sample_masks = next(iter(val_loader))
        sample_imgs = sample_imgs.to(device)
        with torch.no_grad():
            segmentation,aux2, aux3, aux4 = model(sample_imgs)
            
            # ç¡®ä¿åˆ†å‰²ç»“æœä¸è¾“å…¥å›¾åƒå°ºå¯¸ç›¸åŒ
            if segmentation.shape[-2:] != sample_imgs.shape[-2:]:
                segmentation = F.interpolate(
                    segmentation,
                    size=sample_imgs.shape[-2:],
                    mode='bilinear',
                    align_corners=False
                )
            
            sample_probs = torch.sigmoid(segmentation)
            sample_preds = (sample_probs > 0.5).float()

        sample_masks = sample_masks.to(device)
        
        # ç¡®ä¿æ‰€æœ‰å›¾åƒå…·æœ‰ç›¸åŒçš„å°ºå¯¸
        composites = []
        for i in range(min(4, sample_imgs.size(0))):
            img = sample_imgs[i]  # [1, H, W]
            msk = sample_masks[i]  # [1, H, W]
            pred = sample_preds[i]  # [1, H, W]
            
            # ç¡®ä¿æ‰€æœ‰å›¾åƒéƒ½å…·æœ‰ç›¸åŒçš„å¤§å°
            target_size = img.shape[-2:]
            if msk.shape[-2:] != target_size:
                msk = F.interpolate(msk.unsqueeze(0), size=target_size, mode='nearest').squeeze(0)
            if pred.shape[-2:] != target_size:
                pred = F.interpolate(pred.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0)
            
            # è¿æ¥å›¾åƒ
            comp = torch.cat([img, msk, pred], dim=2)  # åœ¨å®½åº¦ç»´åº¦ä¸Šè¿æ¥
            composites.append(comp)

        grid = torch.stack(composites, dim=0)
        vutils.save_image(grid, image_save_template.format(epoch), nrow=2, normalize=True, scale_each=True)

    plot_loss_curve(log_csv, output_path=loss_plot_path, show_head=False)
    last_model_path = os.path.join(log_dir, "model_last.pth")
    torch.save(model.state_dict(), last_model_path)
    print(f"ğŸ’¾ Saved last model to {last_model_path}")

print(f"âœ… Training complete! Best val loss: {best_val_loss:.4f}")



